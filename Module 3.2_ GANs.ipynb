{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"GANs.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"k41yObzZqnZR","colab_type":"text"},"source":["##GANs for Synthetic Data Generation\n","\n","We implement a Generative Adversarial Network (GAN) in the Keras functional API. In this module we will pay attention to:\n","\n","- Implementing the GAN architecture in Keras.\n","- Training our model.\n","- Using our model to generate synthetic data.\n","\n","Note that we will not spend time tuning hyper-parameters: The purpose is to show how different techniques can be implemented in Keras, not to solve particular data science problems as optimally as possible. Obviously, most techniques include hyper-parameters that need to be tuned for optimal performance."]},{"cell_type":"markdown","metadata":{"id":"CNgL43-KrJgf","colab_type":"text"},"source":["We import required libraries."]},{"cell_type":"code","metadata":{"id":"KVH7UR5EqnZU","colab_type":"code","outputId":"5d2a5de4-3718-472b-8bd0-b44fa20ecd7b","executionInfo":{"status":"ok","timestamp":1571345535780,"user_tz":-120,"elapsed":2417,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.datasets.mnist import load_data\n","from keras import Model\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","from keras.layers import Input,Dense,Conv2D,Flatten,Dropout,LeakyReLU,Reshape,Conv2DTranspose\n","from keras.utils.vis_utils import model_to_dot\n","\n","from matplotlib import pyplot\n","\n","import numpy as np\n","import numpy.random as rng\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7dMkvBeRrZpd","colab_type":"text"},"source":["As in the denoising auto-encoder module, we will be working with the MNIST data. These are 28x28 greyscale images of handwritten digits (0-9). The classes are the digit. \n","\n","This data is included in Keras.datasets library, so it is easy to load. We will expand the image dimension so that we have a single channel (as it is greyscale we have only one channel), and normalize the pixel values to real numbers between 0 and 1.\n","\n","We will only work with the training data, and so will create a function that pre-processes and returns that. If you want to look at the data, examine the code in module 2.4, as this provides functions for viewing the MNIST images with their class labels."]},{"cell_type":"code","metadata":{"id":"5f5XCjFuqnZa","colab_type":"code","colab":{}},"source":["def get_data ():\n","  # load the images into memory\n","  (trainX, trainy), (testX, testy) = load_data()\n","  # We will only really work with the trainX data.\n","  # Let's expand it to add channel dimension\n","  X = np.expand_dims(trainX, axis=-1)\n","  # Convert from unsigned ints to floats and scale to between 0 and 1.\n","  X = X.astype('float32')\n","  X = X / 255.0\n","  return X"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8xWZXMXBs1l4","colab_type":"text"},"source":["We call our get data function."]},{"cell_type":"code","metadata":{"id":"MRFxVACts367","colab_type":"code","outputId":"e52b3876-0b85-48a3-ff82-3ea6fb95eaed","executionInfo":{"status":"ok","timestamp":1571345541433,"user_tz":-120,"elapsed":1812,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["X=get_data()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iRzl8kQas8bd","colab_type":"text"},"source":["We want our discriminator to train on batches of real and fake images, learning to distinguish between the two. So we need a function to grab a set of real images from our MNIST image data."]},{"cell_type":"code","metadata":{"id":"b1qqqMVbqnZg","colab_type":"code","colab":{}},"source":["# select real samples\n","def select_real_images(dataset, n_samples):\n","    # choose random instances\n","    indices = rng.randint(0, dataset.shape[0], n_samples)\n","    # retrieve selected images\n","    X = dataset[indices]\n","    # generate 'real' class labels (1)\n","    y = np.ones((n_samples, 1))\n","    return X, y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJz1IZLPtQRf","colab_type":"text"},"source":["Now let us define a function to create our discriminator model. We will also compile it within the function, so if you want to play around with the optimizer change the code in here."]},{"cell_type":"code","metadata":{"id":"0D9l2VbrqnZi","colab_type":"code","colab":{}},"source":["def get_discriminator(in_shape=(28,28,1)):\n","  # Define Model\n","  # This is a basic binary classification CNN\n","  inputs=Input(in_shape)\n","  \n","  conv1=Conv2D(64, (3,3), strides=(2, 2), padding='same')(inputs)\n","  leak1=LeakyReLU(alpha=.2)(conv1)\n","  drop1=Dropout(.4)(leak1)\n","  \n","  conv2=Conv2D(64, (3,3), strides=(2, 2), padding='same')(drop1)\n","  leak2=LeakyReLU(alpha=.2)(conv2)\n","  drop2=Dropout(.4)(leak2)\n","  \n","  flat=Flatten()(drop2)\n","  outputs=Dense(1,activation='sigmoid')(flat)\n","  \n","  # Create Model\n","  model=Model(inputs=inputs,outputs=outputs)\n","  \n","  # Compile model\n","  opt = Adam(lr=0.0002, beta_1=0.5)\n","  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sPd9LvFcxjkQ","colab_type":"text"},"source":["If you wish, run the following code block to create an instance of the discriminator model in order to view a summary.\n","\n","Note we will create the discriminator instance we use for the problem in a later code block."]},{"cell_type":"code","metadata":{"id":"PRHyHrftqnZk","colab_type":"code","outputId":"95c72e62-9b76-4a7f-c970-92aa6238ee50","executionInfo":{"status":"ok","timestamp":1571345552406,"user_tz":-120,"elapsed":723,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":799}},"source":["# Get the discriminator\n","discriminator = get_discriminator()\n","# View a summary of the discriminator\n","discriminator.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 28, 28, 1)         0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 14, 14, 64)        640       \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 3136)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 3137      \n","=================================================================\n","Total params: 40,705\n","Trainable params: 40,705\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FZH_OmoaqnZn","colab_type":"code","colab":{}},"source":["def get_generator(noise_dim=100):\n","    # Define the generator model\n","    inputs=Input((noise_dim,))\n","    \n","    # Create a basis for 7x7 image\n","    # We are using the noise to generate 128 high level 'feature maps' in a 7x7 space\n","    n = 128 * 7 * 7\n","    dense = Dense(n)(inputs)\n","    leak1 = LeakyReLU(alpha=0.2)(dense)\n","    reshape = Reshape((7, 7, 128))(leak1)\n","    \n","    # Now we extract lower level features...\n","    # Upsample to 14x14\n","    conv_tran1 = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(reshape)\n","    leak2=LeakyReLU(alpha=0.2)(conv_tran1)\n","    \n","    # Now we extract even lower level features...\n","    # Upsample to 28x28\n","    conv_tran2 = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(leak2)\n","    leak3 = LeakyReLU(alpha=0.2)(conv_tran2)\n","    \n","    # Finally we put togeather our pixel values for the synthetic image\n","    outputs = Conv2D(1, (7,7), activation='sigmoid', padding='same')(leak3)\n","    \n","    # Create the model and return it\n","    model=Model(inputs=inputs,outputs=outputs)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RfKSwEBBxIgM","colab_type":"text"},"source":["If you wish, run the following code block to create an instance of the generator model in order to view a summary. You can change the dimension of the noise vector if you like.\n","\n","Note we will create the generator instance we use for the problem in a later code block."]},{"cell_type":"code","metadata":{"id":"t7sxLrevqnZp","colab_type":"code","outputId":"8dc0ce69-3f55-41e9-ccb0-36af76ad5c2c","executionInfo":{"status":"ok","timestamp":1571345569796,"user_tz":-120,"elapsed":715,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":467}},"source":["# Specify the dimension of the noise vector\n","noise_dim = 100\n","# Get the generator model\n","generator = get_generator(noise_dim)\n","# Summarize the model\n","generator.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 100)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 6272)              633472    \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 6272)              0         \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       262272    \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","conv2d_transpose_2 (Conv2DTr (None, 28, 28, 128)       262272    \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 28, 28, 128)       0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 28, 28, 1)         6273      \n","=================================================================\n","Total params: 1,164,289\n","Trainable params: 1,164,289\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zU3AIrmCqnZu","colab_type":"text"},"source":["Obviously we want to generate this noise from something simple so that we can sample from the same distribution later. So we use simple independent Gaussian noise.\n","\n","We create a function that will generate as many of these noise vectors as we wish."]},{"cell_type":"code","metadata":{"id":"e833MKtMqnZv","colab_type":"code","colab":{}},"source":["# Generate independent Gaussian noise as input for the generator\n","def generate_input_noise(noise_dim, num_vectors):\n","    # Generate independent Gaussian noise\n","    noise_input = rng.standard_normal(noise_dim * num_vectors)\n","    # Reshape into a 2d array\n","    noise_input = noise_input.reshape(num_vectors, noise_dim)\n","    return noise_input"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfkn3IuQyv4U","colab_type":"text"},"source":["Now we create a function to generate fake images from the noise vectors using the generator. We will also get this function to return labels indicating that the images are fake (we represent this by 0)."]},{"cell_type":"code","metadata":{"id":"u-HS7E0DyrzR","colab_type":"code","colab":{}},"source":["# Generate n fake images using the generator\n","def generate_fake_images(generator, noise_dim, n_images):\n","    # Generate noise vectors\n","    noise = generate_input_noise(noise_dim, n_images)\n","    # Use generator to create fake images from noise vectors\n","    fakes = generator.predict(noise)\n","    # create 'fake' class labels (0)\n","    labels = np.zeros((n_images, 1))\n","    return fakes, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rvHT-HVz8I_","colab_type":"text"},"source":["Let's also create a function that plot's fake images created by the generator so we look at them."]},{"cell_type":"code","metadata":{"id":"8oQgZUJtywl6","colab_type":"code","colab":{}},"source":["# Plot fake images created by generator from random noise\n","def plot_generated_images(generator,noise_dim=100,n_images=25):\n","    # Create the fake images (we don't need the labels here)\n","    X, _ = generate_fake_images(generator, noise_dim, n_images)\n","    # Plot the fake images\n","    for i in range(n_images):\n","        pyplot.subplot(5, 5, 1 + i)\n","        pyplot.axis('off')\n","        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n","    pyplot.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AzIYEk7d1PcE","colab_type":"text"},"source":["Now we will create a function to make our GAN. This will involve putting the discriminator and generator together into a single model. We will also want to be able to treat them as individual models as well, so we will return (1) the discriminator, (2) the generator, and (3) the combined GAN.\n","\n","Note that we will make the weights in the discriminator non-trainable in the combined GAN model. This means that when we train the GAN we are actually only optimizing the weights in the generator. I discuss this further after we define the train function below.\n","\n","Once again, we will compile the model in the function. This is the optimizer that will be used for training the generator (because of the above). If you want to play around with the optimizer change the code inside the function."]},{"cell_type":"code","metadata":{"id":"P-nlZB3vqnZx","colab_type":"code","colab":{}},"source":["# Create the discriminator, generator and GAN networks.\n","# Compile networks requiring compilation within function.\n","def get_gan_models(noise_dim=100):\n","    # Create the discriminator (compiled internally)\n","    discriminator = get_discriminator()\n","    # Create the generator\n","    generator = get_generator(noise_dim)\n","    \n","    # Make weights in the discriminator not trainable\n","    discriminator.trainable = False\n","    \n","    # Make GAN\n","    inputs = Input((noise_dim,))\n","    gan = Model(inputs,discriminator(generator(inputs)))\n","    \n","    # Compile model\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    gan.compile(loss='binary_crossentropy', optimizer=opt)\n","    \n","    return discriminator,generator,gan"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OE43_V3L4So7","colab_type":"text"},"source":["Now let's finally get our networks. If you want to change the noise dimension we work with in the problem, change the value here."]},{"cell_type":"code","metadata":{"id":"Idpsovn34WHw","colab_type":"code","colab":{}},"source":["noise_dim=100\n","discriminator,generator,gan=get_gan_models(noise_dim)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NH6IC5KBgMto","colab_type":"text"},"source":["We are going to code a manual implementation of the training loop. This is so we can implement appropriate training for the discriminater and generator.\n","\n","Note we do not use validation data, and such data makes little sense in this context."]},{"cell_type":"code","metadata":{"id":"vgWX8PzE3-JM","colab_type":"code","colab":{}},"source":["# Train the generator and discriminator\n","def train(\n","    gan, \n","    generator,\n","    discriminator,\n","    dataset, \n","    noise_dim=100, \n","    epochs=100, \n","    batch_size=256\n","):\n","    # We keep the amount of work done per 'epoch' matching\n","    # the dataset size, though in reality this is quite arbitrary.\n","    batches_per_epoch = int(dataset.shape[0] / batch_size)\n","    half_batch = int(batch_size / 2)\n","    # Iterate through desired number of epochs\n","    for i in range(epochs):\n","        # Iterate through batches \n","        for j in range(batches_per_epoch):\n","            # Train the discriminator            \n","            # Get randomly selected real images\n","            X_real, Y_real = select_real_images(dataset, half_batch)\n","            # Generate fake images\n","            X_fake, Y_fake = generate_fake_images(generator, noise_dim, half_batch)\n","            # Create training set for the discriminator\n","            X, Y = np.vstack((X_real, X_fake)), np.vstack((Y_real, Y_fake))\n","            # Update discriminator model weights\n","            d_loss, _ = discriminator.train_on_batch(X, Y)\n","            \n","            # Train the generator\n","            # Prepare input noise\n","            X_gan = generate_input_noise(noise_dim, batch_size)\n","            # Create inverted labels for the fake samples\n","            # This is because the GAN/generator is trying to get the discriminator\n","            # to classify these fake images as real.\n","            Y_gan = np.ones((batch_size, 1))\n","            # Update the generator weights via the discriminator's error\n","            # Remember the discriminator's weights are fixed in the GAN,\n","            # so these will not be adjusted.\n","            g_loss = gan.train_on_batch(X_gan, Y_gan)\n","            \n","            # Give information about loss on this batch\n","            print(\"Epoch: {} Batch: {}/{} Disc. Loss: {:06.5f} Gen. Loss: {:06.5f}\".format(i+1, j+1,batches_per_epoch, d_loss, g_loss))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-TLtIfeNqnZz","colab_type":"text"},"source":["Making the discriminator not trainable is a clever trick in the Keras API.\n","\n","The trainable property impacts the model after it is compiled. The discriminator model was compiled with trainable layers, therefore the model weights in those layers will be updated when the standalone model is updated via calls to the train_on_batch() function.\n","\n","The discriminator model was then marked as not trainable, added to the GAN model, and compiled. In this model, the model weights of the discriminator model are not trainable and cannot be changed when the GAN model is updated via calls to the train_on_batch() function. This change in the trainable property does not impact the training of standalone discriminator model.\n","\n","This behavior is described in the Keras API documentation here:\n","\n","[How can I “freeze” Keras layers?](https://keras.io/getting-started/faq/#how-can-i-freeze-keras-layers)"]},{"cell_type":"code","metadata":{"id":"JAOnVd-AqnZz","colab_type":"code","outputId":"6fa12715-dd37-4584-cd42-3bf74503eecf","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train(gan,generator,discriminator,X)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 Batch: 1/234 Disc. Loss: 0.48953 Gen. Loss: 0.70332\n","Epoch: 1 Batch: 2/234 Disc. Loss: 0.47776 Gen. Loss: 0.70465\n","Epoch: 1 Batch: 3/234 Disc. Loss: 0.46865 Gen. Loss: 0.70561\n","Epoch: 1 Batch: 4/234 Disc. Loss: 0.46272 Gen. Loss: 0.70645\n","Epoch: 1 Batch: 5/234 Disc. Loss: 0.44233 Gen. Loss: 0.70753\n","Epoch: 1 Batch: 6/234 Disc. Loss: 0.44522 Gen. Loss: 0.70848\n","Epoch: 1 Batch: 7/234 Disc. Loss: 0.42647 Gen. Loss: 0.70979\n","Epoch: 1 Batch: 8/234 Disc. Loss: 0.42638 Gen. Loss: 0.71071\n","Epoch: 1 Batch: 9/234 Disc. Loss: 0.42366 Gen. Loss: 0.71154\n","Epoch: 1 Batch: 10/234 Disc. Loss: 0.40530 Gen. Loss: 0.71281\n","Epoch: 1 Batch: 11/234 Disc. Loss: 0.41165 Gen. Loss: 0.71479\n","Epoch: 1 Batch: 12/234 Disc. Loss: 0.39648 Gen. Loss: 0.71673\n","Epoch: 1 Batch: 13/234 Disc. Loss: 0.39200 Gen. Loss: 0.71813\n","Epoch: 1 Batch: 14/234 Disc. Loss: 0.39030 Gen. Loss: 0.72034\n","Epoch: 1 Batch: 15/234 Disc. Loss: 0.39065 Gen. Loss: 0.72285\n","Epoch: 1 Batch: 16/234 Disc. Loss: 0.37975 Gen. Loss: 0.72469\n","Epoch: 1 Batch: 17/234 Disc. Loss: 0.37745 Gen. Loss: 0.72721\n","Epoch: 1 Batch: 18/234 Disc. Loss: 0.37076 Gen. Loss: 0.72963\n","Epoch: 1 Batch: 19/234 Disc. Loss: 0.36893 Gen. Loss: 0.73297\n","Epoch: 1 Batch: 20/234 Disc. Loss: 0.36769 Gen. Loss: 0.73597\n","Epoch: 1 Batch: 21/234 Disc. Loss: 0.36006 Gen. Loss: 0.73839\n","Epoch: 1 Batch: 22/234 Disc. Loss: 0.35650 Gen. Loss: 0.74314\n","Epoch: 1 Batch: 23/234 Disc. Loss: 0.35503 Gen. Loss: 0.74782\n","Epoch: 1 Batch: 24/234 Disc. Loss: 0.35429 Gen. Loss: 0.75143\n","Epoch: 1 Batch: 25/234 Disc. Loss: 0.34946 Gen. Loss: 0.75545\n","Epoch: 1 Batch: 26/234 Disc. Loss: 0.34400 Gen. Loss: 0.76073\n","Epoch: 1 Batch: 27/234 Disc. Loss: 0.34194 Gen. Loss: 0.76678\n","Epoch: 1 Batch: 28/234 Disc. Loss: 0.33919 Gen. Loss: 0.77229\n","Epoch: 1 Batch: 29/234 Disc. Loss: 0.33632 Gen. Loss: 0.77874\n","Epoch: 1 Batch: 30/234 Disc. Loss: 0.33328 Gen. Loss: 0.78537\n","Epoch: 1 Batch: 31/234 Disc. Loss: 0.32284 Gen. Loss: 0.79344\n","Epoch: 1 Batch: 32/234 Disc. Loss: 0.32222 Gen. Loss: 0.80152\n","Epoch: 1 Batch: 33/234 Disc. Loss: 0.31896 Gen. Loss: 0.80983\n","Epoch: 1 Batch: 34/234 Disc. Loss: 0.31609 Gen. Loss: 0.81981\n","Epoch: 1 Batch: 35/234 Disc. Loss: 0.30933 Gen. Loss: 0.83241\n","Epoch: 1 Batch: 36/234 Disc. Loss: 0.30174 Gen. Loss: 0.84218\n","Epoch: 1 Batch: 37/234 Disc. Loss: 0.30202 Gen. Loss: 0.85366\n","Epoch: 1 Batch: 38/234 Disc. Loss: 0.29448 Gen. Loss: 0.86689\n","Epoch: 1 Batch: 39/234 Disc. Loss: 0.28857 Gen. Loss: 0.88253\n","Epoch: 1 Batch: 40/234 Disc. Loss: 0.28684 Gen. Loss: 0.89816\n","Epoch: 1 Batch: 41/234 Disc. Loss: 0.27652 Gen. Loss: 0.91313\n","Epoch: 1 Batch: 42/234 Disc. Loss: 0.27143 Gen. Loss: 0.92950\n","Epoch: 1 Batch: 43/234 Disc. Loss: 0.26984 Gen. Loss: 0.95325\n","Epoch: 1 Batch: 44/234 Disc. Loss: 0.25607 Gen. Loss: 0.97088\n","Epoch: 1 Batch: 45/234 Disc. Loss: 0.25343 Gen. Loss: 0.99215\n","Epoch: 1 Batch: 46/234 Disc. Loss: 0.24489 Gen. Loss: 1.02006\n","Epoch: 1 Batch: 47/234 Disc. Loss: 0.23565 Gen. Loss: 1.04407\n","Epoch: 1 Batch: 48/234 Disc. Loss: 0.22849 Gen. Loss: 1.06920\n","Epoch: 1 Batch: 49/234 Disc. Loss: 0.22802 Gen. Loss: 1.09925\n","Epoch: 1 Batch: 50/234 Disc. Loss: 0.21760 Gen. Loss: 1.12913\n","Epoch: 1 Batch: 51/234 Disc. Loss: 0.20747 Gen. Loss: 1.16254\n","Epoch: 1 Batch: 52/234 Disc. Loss: 0.20080 Gen. Loss: 1.20236\n","Epoch: 1 Batch: 53/234 Disc. Loss: 0.19266 Gen. Loss: 1.23344\n","Epoch: 1 Batch: 54/234 Disc. Loss: 0.18492 Gen. Loss: 1.27521\n","Epoch: 1 Batch: 55/234 Disc. Loss: 0.17802 Gen. Loss: 1.31814\n","Epoch: 1 Batch: 56/234 Disc. Loss: 0.17180 Gen. Loss: 1.36160\n","Epoch: 1 Batch: 57/234 Disc. Loss: 0.16166 Gen. Loss: 1.40139\n","Epoch: 1 Batch: 58/234 Disc. Loss: 0.15277 Gen. Loss: 1.44491\n","Epoch: 1 Batch: 59/234 Disc. Loss: 0.14790 Gen. Loss: 1.49803\n","Epoch: 1 Batch: 60/234 Disc. Loss: 0.14055 Gen. Loss: 1.53509\n","Epoch: 1 Batch: 61/234 Disc. Loss: 0.13532 Gen. Loss: 1.59371\n","Epoch: 1 Batch: 62/234 Disc. Loss: 0.12660 Gen. Loss: 1.64349\n","Epoch: 1 Batch: 63/234 Disc. Loss: 0.12218 Gen. Loss: 1.69124\n","Epoch: 1 Batch: 64/234 Disc. Loss: 0.11517 Gen. Loss: 1.74175\n","Epoch: 1 Batch: 65/234 Disc. Loss: 0.11075 Gen. Loss: 1.79931\n","Epoch: 1 Batch: 66/234 Disc. Loss: 0.10071 Gen. Loss: 1.84275\n","Epoch: 1 Batch: 67/234 Disc. Loss: 0.10020 Gen. Loss: 1.90670\n","Epoch: 1 Batch: 68/234 Disc. Loss: 0.09674 Gen. Loss: 1.94982\n","Epoch: 1 Batch: 69/234 Disc. Loss: 0.08906 Gen. Loss: 2.00343\n","Epoch: 1 Batch: 70/234 Disc. Loss: 0.08604 Gen. Loss: 2.05912\n","Epoch: 1 Batch: 71/234 Disc. Loss: 0.07830 Gen. Loss: 2.11795\n","Epoch: 1 Batch: 72/234 Disc. Loss: 0.07537 Gen. Loss: 2.16180\n","Epoch: 1 Batch: 73/234 Disc. Loss: 0.07429 Gen. Loss: 2.21519\n","Epoch: 1 Batch: 74/234 Disc. Loss: 0.06769 Gen. Loss: 2.25504\n","Epoch: 1 Batch: 75/234 Disc. Loss: 0.06948 Gen. Loss: 2.30493\n","Epoch: 1 Batch: 76/234 Disc. Loss: 0.05593 Gen. Loss: 2.36938\n","Epoch: 1 Batch: 77/234 Disc. Loss: 0.06060 Gen. Loss: 2.41906\n","Epoch: 1 Batch: 78/234 Disc. Loss: 0.05906 Gen. Loss: 2.45384\n","Epoch: 1 Batch: 79/234 Disc. Loss: 0.05310 Gen. Loss: 2.49559\n","Epoch: 1 Batch: 80/234 Disc. Loss: 0.05635 Gen. Loss: 2.54185\n","Epoch: 1 Batch: 81/234 Disc. Loss: 0.05371 Gen. Loss: 2.59948\n","Epoch: 1 Batch: 82/234 Disc. Loss: 0.04758 Gen. Loss: 2.63715\n","Epoch: 1 Batch: 83/234 Disc. Loss: 0.04623 Gen. Loss: 2.68346\n","Epoch: 1 Batch: 84/234 Disc. Loss: 0.05365 Gen. Loss: 2.72491\n","Epoch: 1 Batch: 85/234 Disc. Loss: 0.04372 Gen. Loss: 2.76144\n","Epoch: 1 Batch: 86/234 Disc. Loss: 0.03900 Gen. Loss: 2.80970\n","Epoch: 1 Batch: 87/234 Disc. Loss: 0.04203 Gen. Loss: 2.85787\n","Epoch: 1 Batch: 88/234 Disc. Loss: 0.04256 Gen. Loss: 2.87849\n","Epoch: 1 Batch: 89/234 Disc. Loss: 0.03773 Gen. Loss: 2.92830\n","Epoch: 1 Batch: 90/234 Disc. Loss: 0.03424 Gen. Loss: 2.96724\n","Epoch: 1 Batch: 91/234 Disc. Loss: 0.03437 Gen. Loss: 3.00110\n","Epoch: 1 Batch: 92/234 Disc. Loss: 0.03214 Gen. Loss: 3.02739\n","Epoch: 1 Batch: 93/234 Disc. Loss: 0.03674 Gen. Loss: 3.07481\n","Epoch: 1 Batch: 94/234 Disc. Loss: 0.03191 Gen. Loss: 3.07622\n","Epoch: 1 Batch: 95/234 Disc. Loss: 0.03365 Gen. Loss: 3.06820\n","Epoch: 1 Batch: 96/234 Disc. Loss: 0.05384 Gen. Loss: 2.72563\n","Epoch: 1 Batch: 97/234 Disc. Loss: 1.30907 Gen. Loss: 0.35243\n","Epoch: 1 Batch: 98/234 Disc. Loss: 4.83546 Gen. Loss: 0.00149\n","Epoch: 1 Batch: 99/234 Disc. Loss: 4.59463 Gen. Loss: 0.00424\n","Epoch: 1 Batch: 100/234 Disc. Loss: 3.24542 Gen. Loss: 0.04842\n","Epoch: 1 Batch: 101/234 Disc. Loss: 1.92657 Gen. Loss: 0.44245\n","Epoch: 1 Batch: 102/234 Disc. Loss: 0.90040 Gen. Loss: 1.56990\n","Epoch: 1 Batch: 103/234 Disc. Loss: 0.62692 Gen. Loss: 2.52224\n","Epoch: 1 Batch: 104/234 Disc. Loss: 0.56897 Gen. Loss: 2.88156\n","Epoch: 1 Batch: 105/234 Disc. Loss: 0.51766 Gen. Loss: 2.75907\n","Epoch: 1 Batch: 106/234 Disc. Loss: 0.45010 Gen. Loss: 2.46870\n","Epoch: 1 Batch: 107/234 Disc. Loss: 0.41503 Gen. Loss: 2.28014\n","Epoch: 1 Batch: 108/234 Disc. Loss: 0.38968 Gen. Loss: 2.12091\n","Epoch: 1 Batch: 109/234 Disc. Loss: 0.38284 Gen. Loss: 2.35943\n","Epoch: 1 Batch: 110/234 Disc. Loss: 0.33993 Gen. Loss: 2.52047\n","Epoch: 1 Batch: 111/234 Disc. Loss: 0.31958 Gen. Loss: 2.75611\n","Epoch: 1 Batch: 112/234 Disc. Loss: 0.28499 Gen. Loss: 2.68533\n","Epoch: 1 Batch: 113/234 Disc. Loss: 0.29012 Gen. Loss: 2.53785\n","Epoch: 1 Batch: 114/234 Disc. Loss: 0.27591 Gen. Loss: 2.49891\n","Epoch: 1 Batch: 115/234 Disc. Loss: 0.26780 Gen. Loss: 2.31399\n","Epoch: 1 Batch: 116/234 Disc. Loss: 0.30435 Gen. Loss: 2.24304\n","Epoch: 1 Batch: 117/234 Disc. Loss: 0.28164 Gen. Loss: 2.04860\n","Epoch: 1 Batch: 118/234 Disc. Loss: 0.31182 Gen. Loss: 1.97909\n","Epoch: 1 Batch: 119/234 Disc. Loss: 0.32473 Gen. Loss: 1.96299\n","Epoch: 1 Batch: 120/234 Disc. Loss: 0.31128 Gen. Loss: 2.01563\n","Epoch: 1 Batch: 121/234 Disc. Loss: 0.32382 Gen. Loss: 2.19891\n","Epoch: 1 Batch: 122/234 Disc. Loss: 0.29466 Gen. Loss: 1.94064\n","Epoch: 1 Batch: 123/234 Disc. Loss: 0.39478 Gen. Loss: 1.45022\n","Epoch: 1 Batch: 124/234 Disc. Loss: 0.41771 Gen. Loss: 1.63825\n","Epoch: 1 Batch: 125/234 Disc. Loss: 0.39653 Gen. Loss: 1.43634\n","Epoch: 1 Batch: 126/234 Disc. Loss: 0.43994 Gen. Loss: 1.29738\n","Epoch: 1 Batch: 127/234 Disc. Loss: 0.47581 Gen. Loss: 1.41286\n","Epoch: 1 Batch: 128/234 Disc. Loss: 0.47333 Gen. Loss: 1.35005\n","Epoch: 1 Batch: 129/234 Disc. Loss: 0.51462 Gen. Loss: 1.25402\n","Epoch: 1 Batch: 130/234 Disc. Loss: 0.56502 Gen. Loss: 1.20448\n","Epoch: 1 Batch: 131/234 Disc. Loss: 0.55558 Gen. Loss: 1.13527\n","Epoch: 1 Batch: 132/234 Disc. Loss: 0.56762 Gen. Loss: 1.03786\n","Epoch: 1 Batch: 133/234 Disc. Loss: 0.58698 Gen. Loss: 1.00792\n","Epoch: 1 Batch: 134/234 Disc. Loss: 0.63043 Gen. Loss: 0.95436\n","Epoch: 1 Batch: 135/234 Disc. Loss: 0.64223 Gen. Loss: 0.92283\n","Epoch: 1 Batch: 136/234 Disc. Loss: 0.65417 Gen. Loss: 0.87958\n","Epoch: 1 Batch: 137/234 Disc. Loss: 0.70327 Gen. Loss: 0.89124\n","Epoch: 1 Batch: 138/234 Disc. Loss: 0.70255 Gen. Loss: 0.88550\n","Epoch: 1 Batch: 139/234 Disc. Loss: 0.66543 Gen. Loss: 0.89445\n","Epoch: 1 Batch: 140/234 Disc. Loss: 0.67339 Gen. Loss: 0.92381\n","Epoch: 1 Batch: 141/234 Disc. Loss: 0.65574 Gen. Loss: 0.93214\n","Epoch: 1 Batch: 142/234 Disc. Loss: 0.68736 Gen. Loss: 0.98720\n","Epoch: 1 Batch: 143/234 Disc. Loss: 0.62085 Gen. Loss: 0.93914\n","Epoch: 1 Batch: 144/234 Disc. Loss: 0.61146 Gen. Loss: 0.98197\n","Epoch: 1 Batch: 145/234 Disc. Loss: 0.62185 Gen. Loss: 0.97989\n","Epoch: 1 Batch: 146/234 Disc. Loss: 0.57647 Gen. Loss: 1.03989\n","Epoch: 1 Batch: 147/234 Disc. Loss: 0.57485 Gen. Loss: 1.09471\n","Epoch: 1 Batch: 148/234 Disc. Loss: 0.53874 Gen. Loss: 1.10402\n","Epoch: 1 Batch: 149/234 Disc. Loss: 0.51802 Gen. Loss: 1.09569\n","Epoch: 1 Batch: 150/234 Disc. Loss: 0.51865 Gen. Loss: 1.13654\n","Epoch: 1 Batch: 151/234 Disc. Loss: 0.53154 Gen. Loss: 1.13616\n","Epoch: 1 Batch: 152/234 Disc. Loss: 0.48874 Gen. Loss: 1.10656\n","Epoch: 1 Batch: 153/234 Disc. Loss: 0.49008 Gen. Loss: 1.19593\n","Epoch: 1 Batch: 154/234 Disc. Loss: 0.45478 Gen. Loss: 1.21029\n","Epoch: 1 Batch: 155/234 Disc. Loss: 0.42018 Gen. Loss: 1.25585\n","Epoch: 1 Batch: 156/234 Disc. Loss: 0.41466 Gen. Loss: 1.24879\n","Epoch: 1 Batch: 157/234 Disc. Loss: 0.39785 Gen. Loss: 1.30547\n","Epoch: 1 Batch: 158/234 Disc. Loss: 0.39909 Gen. Loss: 1.30757\n","Epoch: 1 Batch: 159/234 Disc. Loss: 0.36610 Gen. Loss: 1.32828\n","Epoch: 1 Batch: 160/234 Disc. Loss: 0.37459 Gen. Loss: 1.34307\n","Epoch: 1 Batch: 161/234 Disc. Loss: 0.37534 Gen. Loss: 1.34398\n","Epoch: 1 Batch: 162/234 Disc. Loss: 0.35506 Gen. Loss: 1.36505\n","Epoch: 1 Batch: 163/234 Disc. Loss: 0.33831 Gen. Loss: 1.40795\n","Epoch: 1 Batch: 164/234 Disc. Loss: 0.32598 Gen. Loss: 1.45478\n","Epoch: 1 Batch: 165/234 Disc. Loss: 0.31853 Gen. Loss: 1.46043\n","Epoch: 1 Batch: 166/234 Disc. Loss: 0.28419 Gen. Loss: 1.47452\n","Epoch: 1 Batch: 167/234 Disc. Loss: 0.29931 Gen. Loss: 1.48511\n","Epoch: 1 Batch: 168/234 Disc. Loss: 0.28454 Gen. Loss: 1.51055\n","Epoch: 1 Batch: 169/234 Disc. Loss: 0.28131 Gen. Loss: 1.52335\n","Epoch: 1 Batch: 170/234 Disc. Loss: 0.28073 Gen. Loss: 1.53088\n","Epoch: 1 Batch: 171/234 Disc. Loss: 0.26906 Gen. Loss: 1.51153\n","Epoch: 1 Batch: 172/234 Disc. Loss: 0.24445 Gen. Loss: 1.52068\n","Epoch: 1 Batch: 173/234 Disc. Loss: 0.26197 Gen. Loss: 1.53163\n","Epoch: 1 Batch: 174/234 Disc. Loss: 0.26168 Gen. Loss: 1.55363\n","Epoch: 1 Batch: 175/234 Disc. Loss: 0.24716 Gen. Loss: 1.55948\n","Epoch: 1 Batch: 176/234 Disc. Loss: 0.23485 Gen. Loss: 1.57621\n","Epoch: 1 Batch: 177/234 Disc. Loss: 0.22657 Gen. Loss: 1.59843\n","Epoch: 1 Batch: 178/234 Disc. Loss: 0.21914 Gen. Loss: 1.59691\n","Epoch: 1 Batch: 179/234 Disc. Loss: 0.22116 Gen. Loss: 1.61734\n","Epoch: 1 Batch: 180/234 Disc. Loss: 0.21841 Gen. Loss: 1.58321\n","Epoch: 1 Batch: 181/234 Disc. Loss: 0.20657 Gen. Loss: 1.59711\n","Epoch: 1 Batch: 182/234 Disc. Loss: 0.20516 Gen. Loss: 1.62531\n","Epoch: 1 Batch: 183/234 Disc. Loss: 0.21328 Gen. Loss: 1.63817\n","Epoch: 1 Batch: 184/234 Disc. Loss: 0.20103 Gen. Loss: 1.67033\n","Epoch: 1 Batch: 185/234 Disc. Loss: 0.21264 Gen. Loss: 1.63770\n","Epoch: 1 Batch: 186/234 Disc. Loss: 0.19816 Gen. Loss: 1.66350\n","Epoch: 1 Batch: 187/234 Disc. Loss: 0.19309 Gen. Loss: 1.66822\n","Epoch: 1 Batch: 188/234 Disc. Loss: 0.18434 Gen. Loss: 1.68446\n","Epoch: 1 Batch: 189/234 Disc. Loss: 0.19282 Gen. Loss: 1.69152\n","Epoch: 1 Batch: 190/234 Disc. Loss: 0.17847 Gen. Loss: 1.71078\n","Epoch: 1 Batch: 191/234 Disc. Loss: 0.17662 Gen. Loss: 1.71375\n","Epoch: 1 Batch: 192/234 Disc. Loss: 0.17315 Gen. Loss: 1.72046\n","Epoch: 1 Batch: 193/234 Disc. Loss: 0.18113 Gen. Loss: 1.75379\n","Epoch: 1 Batch: 194/234 Disc. Loss: 0.16827 Gen. Loss: 1.74386\n","Epoch: 1 Batch: 195/234 Disc. Loss: 0.18173 Gen. Loss: 1.78932\n","Epoch: 1 Batch: 196/234 Disc. Loss: 0.18829 Gen. Loss: 1.76871\n","Epoch: 1 Batch: 197/234 Disc. Loss: 0.17704 Gen. Loss: 1.79785\n","Epoch: 1 Batch: 198/234 Disc. Loss: 0.17011 Gen. Loss: 1.79293\n","Epoch: 1 Batch: 199/234 Disc. Loss: 0.17051 Gen. Loss: 1.80772\n","Epoch: 1 Batch: 200/234 Disc. Loss: 0.15309 Gen. Loss: 1.87442\n","Epoch: 1 Batch: 201/234 Disc. Loss: 0.16914 Gen. Loss: 1.87593\n","Epoch: 1 Batch: 202/234 Disc. Loss: 0.15067 Gen. Loss: 1.91014\n","Epoch: 1 Batch: 203/234 Disc. Loss: 0.16710 Gen. Loss: 1.97732\n","Epoch: 1 Batch: 204/234 Disc. Loss: 0.16484 Gen. Loss: 2.01809\n","Epoch: 1 Batch: 205/234 Disc. Loss: 0.14311 Gen. Loss: 1.99996\n","Epoch: 1 Batch: 206/234 Disc. Loss: 0.15069 Gen. Loss: 2.04159\n","Epoch: 1 Batch: 207/234 Disc. Loss: 0.13612 Gen. Loss: 2.08493\n","Epoch: 1 Batch: 208/234 Disc. Loss: 0.12463 Gen. Loss: 2.09255\n","Epoch: 1 Batch: 209/234 Disc. Loss: 0.15410 Gen. Loss: 2.12064\n","Epoch: 1 Batch: 210/234 Disc. Loss: 0.15006 Gen. Loss: 2.09407\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HWyxm3-eqnZ2","colab_type":"text"},"source":["You may see a warning:\n","\n","UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","\n","Don't worry about this - it is actually doing exactly what we want.\n","\n","When you set certain weights to trainable or untrainable (as we did in the line: discriminator.trainable = False) this only takes effect once you compile the model. We compiled the discriminator model before this call, so its weights are still trainable when we train *the discriminator model* by batch.\n","\n","But the discriminator model is also a *sub-model* of our complete GAN. And the GAN model was compiled *after* we set the discriminator weights to untrainable. So the discriminator weights will not be trained when we train the GAN model by batch. Neat huh?\n","\n","The warning is Keras thinking we have been too clever by half and are possibly making a mistake somewhere. But we aren't. :)\n","\n","You could get rid of the warning by setting the discriminator trainability to True and False and then recompiling models within the training loop. But that would be a lot more work..."]},{"cell_type":"code","metadata":{"id":"vvPiMH3kqnZ2","colab_type":"code","outputId":"cfbfc72e-fdb2-4ff1-d5ff-8e934b19f627","colab":{}},"source":["plot_generated_images(generator,100,25)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 25 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"4XYcN5LEqnZ5","colab_type":"text"},"source":["1. You might be surprised that there are images of all sorts of digits, thinking that it looks easiest for the generator to fake certain digits like '0'. But for any reasonably balanced multi-class problem, the generator cannot focus on simulating the simplest class - otherwise the discriminator will simply 'abandon' that class as typically fake. \n","\n","2. You might be surprised that the loss functions oscillate. But remember the two networks are in competition!\n","\n","3. We can generate synthetic data from the GAN. But unlike the variational auto-encoder, in this naive implementation we cannot generate synthetic data of certain classes (similar to known images)."]}]}