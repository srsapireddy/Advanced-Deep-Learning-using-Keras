{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Module 2.3: LSTMs.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DSDYSPCLIpsR","colab_type":"text"},"source":["## Module 2.3: Working with LSTMs in Keras (A Review)\n","\n","We turn to implementing a type of recurrent neural network know as LSTM in the Keras functional API. In this module we will pay attention to:\n","\n","1. Using the Keras functional API for defining models.\n","2. Mounting your Google drive to your Colab environment for file interface.\n","3. Generating synthetic data from a LSTM and sequence seed.\n","\n","Those students who are comfortable with all these matters might consider skipping ahead.\n","\n","Note that we will not spend time tuning hyper-parameters: The purpose is to show how different techniques can be implemented in Keras, not to solve particular data science problems as optimally as possible. Obviously, most techniques include hyper-parameters that need to be tuned for optimal performance."]},{"cell_type":"markdown","metadata":{"id":"TK1JgxD3LD1U","colab_type":"text"},"source":["First we import required libraries."]},{"cell_type":"code","metadata":{"id":"btVPO4ETIEfL","colab_type":"code","outputId":"25a62dae-91b6-4b76-b83d-53382e4404d4","executionInfo":{"status":"ok","timestamp":1576672451562,"user_tz":-60,"elapsed":2066,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":98}},"source":["import sys\n","import numpy\n","\n","from google.colab import drive\n","\n","from keras.models import Sequential\n","from keras import Model\n","from keras.optimizers import Adadelta\n","from keras.layers import Dense,Dropout,LSTM,Input\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"sGCD1zv5Low8","colab_type":"text"},"source":["We will have a little fun and try to teach a neural network to write like Lewis Carroll, the author of Alice in Wonderland.\n","\n","Note, though, that the same technique can be used to model any sequential system, and generate simulations from seeds for such a system. Here the sequence are the characters written by Carroll during Alice in Wonderland, but it could be, for example, an industrial system that evolves in time. In that case, when we generate simulations of the system based on current and recent conditions we simulate the expected evolution of the system - something of great value!"]},{"cell_type":"markdown","metadata":{"id":"sfYWdsSGQJlQ","colab_type":"text"},"source":["We will use the [Project Gutenburg text file of Alice in Wonderland](https://www.gutenberg.org/files/11/11.txt). But we need to get the file into our colab environment and this takes some work.\n","\n","First, you need to place the file in your google drive. We will assume that you will place it in a folder called \"Mastering Keras Datasets\", and that you rename it \"Alice.txt\". If you don't, you will need to the file path used in the code.\n","\n","Once you have done that, you will need to mount your google drive in Colab. Run the following code and complete the required authorizations.\n","\n","Note that you will need to mount your drive every time you use code from this tutorial."]},{"cell_type":"code","metadata":{"id":"IlKpjJfKcksv","colab_type":"code","outputId":"e57bb726-7439-455b-816d-4c1f442edb0e","executionInfo":{"status":"ok","timestamp":1571045131919,"user_tz":-120,"elapsed":45875,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["# Note: You will need to mount your drive every time you \n","# run code in this tutorial.\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oV3vUZryMUjT","colab_type":"text"},"source":["Now we can load the file using code and prepare the data. We want to work with sequences of 100 characters as input data, and our target will be the next (101st) character.\n","\n","To keep things simple, we will ignore upper/lower case character distinctions, and cast all alphabetical characters to lower case. To allow our model to work with these characters, we will encode them as integers. We will then normalize them to real numbers between 0 and 1 and add a dimension (we are working with a system with a single feature). Finally we will one-hot encode the target character (see previous module for discussion of one-hot encoding). This is not the only way to handle the data, but it is a simple one.\n","\n","We will also return the unnormalized and non-reshaped X data, the number of characters found and an integer coding to character dictionary, all for use later.\n"]},{"cell_type":"code","metadata":{"id":"-RXtz4LwLHoY","colab_type":"code","colab":{}},"source":["def load_alice (\n","    rawTextFile=\"/content/drive/My Drive/Mastering Keras Datasets/Alice.txt\"   \n","    ):\n","    # load ascii text and covert to lowercase\n","    raw_text = open(rawTextFile, encoding='utf-8').read()\n","    raw_text = raw_text.lower()\n","    # create mapping of unique chars to integers\n","    chars = sorted(list(set(raw_text)))\n","    char_to_int = dict((c, i) for i, c in enumerate(chars))\n","    int_to_char = dict((i, c) for i, c in enumerate(chars))\n","    # summarize the loaded data\n","    n_chars = len(raw_text)\n","    n_vocab = len(chars)\n","    print (\"Total Characters: \", n_chars)\n","    print (\"Total Vocab: \", n_vocab)\n","    # prepare the dataset of input to output pairs encoded as integers\n","    seq_length = 100\n","    dataX = []\n","    dataY = []\n","    for i in range(0, n_chars - seq_length, 1):\n","    \tseq_in = raw_text[i:i + seq_length]\n","    \tseq_out = raw_text[i + seq_length]\n","    \tdataX.append([char_to_int[char] for char in seq_in])\n","    \tdataY.append(char_to_int[seq_out])\n","    n_patterns = len(dataX)\n","    print (\"Total Patterns: \", n_patterns)\n","    # reshape X to be [samples, time steps, features]\n","    X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","    # normalize\n","    X = X / float(n_vocab)\n","    # one hot encode the output variable\n","    Y = np_utils.to_categorical(dataY)\n","    return X,Y,dataX,n_vocab,int_to_char"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LikRHGnkM8r-","colab_type":"text"},"source":["Now lets load the data. X and Y are the input and target label datasets we will use in training. X_ is the un-reshaped X data for use later."]},{"cell_type":"code","metadata":{"id":"kXdKny5NM_RT","colab_type":"code","outputId":"25e0e0d9-9daf-4ab2-9fd2-8cd256f775c1","executionInfo":{"status":"ok","timestamp":1571045177877,"user_tz":-120,"elapsed":3721,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["X,Y,X_,n_vocab,int_to_char = load_alice()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total Characters:  163810\n","Total Vocab:  58\n","Total Patterns:  163710\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BHOixFxYdl1H","colab_type":"text"},"source":["You can play around below to look at the shape of the resulting X and Y arrays, as well as their contents. But they are no longer understandable character strings."]},{"cell_type":"code","metadata":{"id":"X2fL5Qbjdk1Z","colab_type":"code","colab":{}},"source":["# Play around here to look at data characteristics"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p3Wk5Dvzexjs","colab_type":"text"},"source":["Now we define our LSTM using the Keras function API. We are going to make use of LSTM layers, and add a dropout layer for regularization.\n","\n","We will pass the data to the model defining function so that we can read input and output dimensions of it, rather than hard coding them.\n","\n","For comparison, a second version of the function is included showing how to use the sequential approach."]},{"cell_type":"code","metadata":{"id":"YpQik6VXIdTl","colab_type":"code","colab":{}},"source":["def get_model (X,Y):\n","    # define the LSTM model\n","    inputs=Input(shape=(X.shape[1],X.shape[2]),name=\"Input\")\n","    lstm1=LSTM(256, input_shape=(100,1),return_sequences=True)(inputs)\n","    drop1=Dropout(0.2)(lstm1)\n","    lstm2=LSTM(256)(drop1)\n","    drop2=Dropout(0.2)(lstm2)\n","    outputs=Dense(Y.shape[1], activation='softmax')(drop2)\n","    model=Model(inputs=inputs,outputs=outputs)\n","    return model\n","\n","def get_model_sequential (X,Y):\n","    # define the LSTM model\n","    model = Sequential()\n","    model.add(LSTM(256, input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\n","    model.add(Dropout(0.2))\n","    model.add(LSTM(256))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(Y.shape[1], activation='softmax'))\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"25ea1_kwhKSL","colab_type":"text"},"source":["We get our model."]},{"cell_type":"code","metadata":{"id":"yHO2WNkIhPz4","colab_type":"code","colab":{}},"source":["model=get_model(X,Y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8AlPqyChSLp","colab_type":"text"},"source":["Now we will define an optimizer and compile it. If you are unfamiliar with the different types of optimizers available in keras, I suggest you read the keras documentation [here](https://keras.io/optimizers/) and play around training the model with different alternatives."]},{"cell_type":"code","metadata":{"id":"D4QI98iFhdRn","colab_type":"code","colab":{}},"source":["opt=Adadelta()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hNvQlYwUhhps","colab_type":"text"},"source":["And we compile our model with the optimizer ready for training. We use categorical crossentropy as our loss function as this is a good default choice for working with a multi-class categorical target variable (i.e. the next character labels)."]},{"cell_type":"code","metadata":{"id":"-gBX1zHbh8T_","colab_type":"code","colab":{}},"source":["model.compile(optimizer=opt,\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WxHPgLnEkZdn","colab_type":"text"},"source":["Now we will make a function to fit the model. We will not do this very professionally (it is just a fun project), and so will not use any validation data. Rather, we will just run the training for a number of epoches - by default 20, though you can change this.\n","\n","We will, though, use a ModelCheckpoint callback to save the best performing weights and load these into the model and the conclusion of the training. Note that training performance should normally improve with more epoches, so this is unlikely to improve performance. What we really want is to be able to load the best weights without having to redo the training process (see below)\n","\n","If you want to, you are encouraged to alter the code in this tutorial to work with a training and validation set, and adjust the fit function below to incorporate an EarlyStopping callback based on performance on the validation data.\n","\n","We have two one LSTM layer, we are dealing with sequences of length 100. So if we 'unroll' it, we have a network of 200 LSTM layers. And inside these layers are infact multiple internal layers setting up the LSTM architecture! So this is actually a pretty big network, and training will take some time (about 200 hours on the free Colab environment for 200 epochs). This is probably too much to conveniently run yourself.\n","\n","Here we have an example of how we could train it on Colab. Colab will eventually time out. The best thing to do is to save our weights file to our google drive, so we can load it at leisure later and resume training. This is what we will do. Remember that if you didn't use the default name for your folder in your google drive you should change the path string in the code.\n","\n","In real life, you will also often want to save the state of the optimizer (so that it keeps its current learning rate, etc). You can do this by accessing and saving model.optimizer.get_state(). It is left as an exercise to implement this.\n","\n","*It is not expected that you train the network using this function - see below to load trained weights from your google drive.*"]},{"cell_type":"code","metadata":{"id":"2W1MXN18kZ3N","colab_type":"code","colab":{}},"source":["def fit_model (model,X,Y,epochs=100):\n","    # define the checkpoint callback\n","    filepath=\"/content/drive/My Drive/Mastering Keras Datasets/alice_best_weights.hdf5\" \n","    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n","                                 save_best_only=True, mode='min')\n","    callbacks_list = [checkpoint]\n","    # fit the model\n","    model.fit(X, Y, epochs=epochs, batch_size=128, callbacks=callbacks_list)\n","    # load the best weights\n","    model.load_weights(filename)\n","    # return the final model\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ek8_BWfrh8fm","colab_type":"text"},"source":["We would then fit (train) the model by calling the above function.\n","\n","*It is not expected that you train the network using this function - see below to load trained weights from your google drive.*"]},{"cell_type":"code","metadata":{"id":"HzsCLeYthGK-","colab_type":"code","outputId":"4b13c24e-78c3-4d98-ac83-1ca72b330017","executionInfo":{"status":"error","timestamp":1570711632825,"user_tz":-120,"elapsed":3247167,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":504}},"source":["model=fit_model(model,X,Y,100)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","163710/163710 [==============================] - 3246s 20ms/step - loss: 3.0840 - acc: 0.1663\n","\n","Epoch 00001: loss improved from inf to 3.08398, saving model to /content/drive/My Drive/Mastering Keras Datasets/alice_best_weights.hdf5\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-b804c79b2fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-23-b6a2d0914385>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, X, Y, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# load the best weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    222\u001b[0m                         \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    717\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_write_to_gcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_wrapper\u001b[0;34m(obj, filepath, overwrite, *args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msave_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproceed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m             \u001b[0m_serialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/content/drive/My Drive/Mastering Keras Datasets/alice_best_weights.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"]}]},{"cell_type":"markdown","metadata":{"id":"clAYq_A81Cob","colab_type":"text"},"source":["Here we will load saved weights. You can use the \"alice_best_weights.hdf5\" file that comes with the course - just place it in the same folder as the \"alice.txt\" file in your google drive. This file has been trained for 200 epoches, and gets a loss around 1.16.\n","\n","If you train the network yourself, the best weights will be saved as \"alice_best_weights.hdf5\" in the same location as above. You can therefore use the same code in both cases.\n","\n","In all cases remember to change the filepath if you are not using the default folder name.\n","\n","If you are resuming this tutorial here in a new session, you should re-mount your Google drive using the earlier code, re-load the data, and then run this code block to load the weights into a new model. \n","\n","If you want to train the model further, you will need to compile it with an optimizer."]},{"cell_type":"code","metadata":{"id":"A7bvAwsEfzlo","colab_type":"code","outputId":"2d48bd39-c481-4caa-e8dd-a49b2b2903cf","executionInfo":{"status":"ok","timestamp":1571043657639,"user_tz":-120,"elapsed":1835,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":435}},"source":["model=get_model(X,Y)\n","filepath=\"/content/drive/My Drive/Mastering Keras Datasets/alice_best_weights.hdf5\"\n","model.load_weights(filepath)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r1x9CiNJ1tjY","colab_type":"text"},"source":["Now we can see if our network has mastered the art of writing like Lewis Carroll! Let's write a function to let us see, and then call it."]},{"cell_type":"code","metadata":{"id":"qC2dzmdpITMG","colab_type":"code","colab":{}},"source":["def write_like_Lewis_Carroll(model,X_,n_vocab,int_to_char):\n","  # pick a random seed...\n","  start = numpy.random.randint(0, len(X_)-1)\n","  # ... in order to decide which X datum to use to start\n","  pattern = X_[start]\n","\n","  print (\"Seed:\")\n","  print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","  # generate characters\n","  for i in range(1000):\n","    # We transform the integer mapping of the characters to\n","    # real numbers suitable for input into our model.\n","    x = numpy.reshape(pattern, (1, len(pattern), 1))\n","    x = x/float(n_vocab)\n","    # We use the model to estimate the probability distribution for\n","    # the next character\n","    prediction = model.predict(x, verbose=0)\n","    # We choose as the next character whichever the model thinks is most likely\n","    index = numpy.argmax(prediction)\n","    result = int_to_char[index]\n","    seq_in = [int_to_char[value] for value in pattern]\n","    sys.stdout.write(result)\n","    # We add the integer to our pattern... \n","    pattern.append(index)\n","    # ... and drop the earliest integer from our pattern.\n","    pattern = pattern[1:len(pattern)]\n","  print (\"\\nDone.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aG-PGS0vISCB","colab_type":"code","outputId":"8896db20-cfed-4a7a-bbfc-c539508b3446","executionInfo":{"status":"ok","timestamp":1571043754022,"user_tz":-120,"elapsed":24907,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":418}},"source":["write_like_Lewis_Carroll(model,X_,n_vocab,int_to_char)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Seed:\n","\" for it to speak with.\n","\n","alice waited till the eyes appeared, and then nodded. 'it's no use\n","speaking t \"\n","o see the mock turtle shat '\n","\n","'i should hiv tereat ' thought alice, 'i must be giederen seams to be a bonk,' she said to herself, 'it would be of very curious to onow what there was a sery dortut, and the ooral of that iss thin the cook and a large rister sha thought the was now one of the court.\n","but the dould not heve a little botrle of the thate with a things of tee the door, she could not hear the conlers on the coor with pisted so see it was she same sotnd and mook up and was that it was ouer the whnle shoiek, and the thought the was now a bot of ceain, and was domencd it voice and bookdrs shat the was nuire silent for a minute, and she was nooiing at the court.\n","\n","'i should hit tere things,' said the caterpillar.\n","\n","'well, perhaps you may bean the same siings tuertion,' the duchess said to the gryphon.\n","\n","'what i cen the thing,' said the caterpillar.\n","\n","'well, perhaps you may bean the same siings tuertion,' the mock turtle seplied,\n","\n","'that i man the mice,' said the caterpillar.\n","\n","'well, per\n","Done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BG_f2DsQgW5_","colab_type":"text"},"source":["If you run the above a few times, you will see that we have had some success - though we are still a long way from a good Alice in Wonderland simulator!\n","\n","Here is an extract from one simulation I ran:\n","\n","*'i should hit tere things,' said the caterpillar.*\n","\n","*'well, perhaps you may bean the same siings tuertion,' the duchess said to the gryphon.*\n","\n","*'what i cen the thing,' said the caterpillar.*\n","\n","*'well, perhaps you may bean the same siings tuertion,' the mock turtle seplied,*\n","\n","*'that i man the mice,' said the caterpillar.*\n","\n","We have got to the point of basic sentence structure, quotations for speech, plausible characters given the context, etc. There remains misspellings, and occasional punctuation errors, and other issues. (And this was a good selection.) \n","\n","In fact, you should be able to do much better. Trying with 500 time points (predicting the 501st character from the preceeding 500) and using a three layer LSTM will lead to major improvements. So would using more training data (multiple Lewis Carole books). You can see the performance achieved on a Shakespeare simulator [here](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n","\n","If you have time, consider it an exercise to try to improve this implementation to that level - but be warned, the suggested changes would lead to training time being about 7 times longer for the same number of epochs, and of course more epoches would be required as it would be a more complex model. Since it would have taken 100+ hours on the Colab environment (which disconnects after a time limit) this is really only an exercise for those with access to a powerful local environment. "]}]}